; This is an example configuration for training machine translation.  It is an
; INI file with few added syntactic restrictions.
;
; Names in square brackets refer to objects in the program. With the exception
; of the [main] block, all of them will be instantiated as objects.
;
; The field values can be of several types:
;
;   * None - interpreted as Python None
;   * True / False - interpreted as boolean values
;   * integers
;   * floating point numbers
;   * Python types (fully defined with module name)
;   * references to other objects in the configuration, closed in <>
;   * strings (if it does not match any other pattern)
;   * list of the previous, enclosed in square brackets, comma-separated
;
; The vocabularies are handled in a special way. If the vocabularies source is
; defined in the [main] (a dataset object), a dictionary that maps the language
; code to the vocabularies is created. Later, if any other block has a field
; called 'vocabulary', and its value is a known language code, the vocabulary
; from the dictionary is used. Vocabularies can also be defined as objects
; in the INI file and can be referenced using the <> notation.
;

[main]
; The main block contains the mandatory fields for running an experiment.
output=out-example-translation
encoders=[<encoder>]
decoder=<decoder>
runner=<runner>
evaluation=[<bleu1>, <bleu4>]
threads=4

name=translation
batch_size=128
epochs=10
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
validation_period=1000
logging_period=20
save_n_best=5
postprocess=<bpe_postprocess>

[bleu1]
class=evaluators.bleu.BLEUEvaluator
n=1

[bleu4]
class=evaluators.bleu.BLEUEvaluator
n=4


[train_data]
; This is a definition of the training data object. Dataset is not a standard
; class, it treats the __init__ method's arguments as a dictionary, therefore
; the data series names can be any string, prefixed with "s_". To specify the
; output file for a series, use "s_" prefix and "_out" suffix, e.g.
; "s_target_out"
class=config.utils.dataset_from_files
s_source=examples/data/train.en
s_tags=examples/data/train.tags.en
s_target=examples/data/train.de
preprocessor=<bpe_preprocess>

[val_data]
; Validation data, the languages are not necessary here, encoders and decoders
; access the data series via the string identifiers defined here.
class=config.utils.dataset_from_files
s_source=examples/data/val.en
s_tags=examples/data/val.tags.en
s_target=examples/data/val.de
preprocessor=<bpe_preprocess>

[source_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=[source]
max_size=50000

[target_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=[target]
max_size=50000

[tags_vocabulary]
class=vocabulary.from_dataset
datasets=[<train_data>]
series_ids=[tags]
max_size=50

[bpe_preprocess]
class=processors.bpe.BPEPreprocessor
merge_file=examples/data/bpe_merges

[encoder]
; This defines the sentence encoder object. All compulsory arguments from the
; __init__ methods must be defined in this block. Additional arguments may
; be defined.  Otherwise, the default value from the __init__ method is
; used. Notice the vocabulary is aquired via the language string.
class=encoders.factored_encoder.FactoredEncoder
rnn_size=300
max_input_len=50
embedding_size=300
dropout_keep_p=0.8
attention_type=decoding_function.Attention
data_ids=[source,tags]
vocabularies=[<source_vocabulary>, <tags_vocabulary>]

[decoder]
class=decoders.decoder.Decoder
encoders=[<encoder>]
rnn_size=256
embedding_size=300
use_attention=True
dropout_keep_p=0.8
data_id=target
vocabulary=<target_vocabulary>
max_output_len=50

[trainer]
; This block just fills the arguments of the trainer __init__ method.
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoder=<decoder>
l2_regularization=1.0e-8

[runner]
; This block is used for both validation and testing to run the model on a given
; dataset.
class=runners.runner.GreedyRunner
decoder=<decoder>
batch_size=256

[bpe_postprocess]
class=processors.bpe.BPEPostprocessor
