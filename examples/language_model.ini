; This is an example configuration for training a language model.  It is an
; INI file with few added syntanctic restrictions.
;
; Names in square brackets refer to objects in the program. With the exception
; of the [main] block, all of them will be instantiated as objects.
;
; The field values can be of several types:
;
;   * None - interpreted as Python None
;   * True / False - interpreted as boolean values
;   * integers
;   * floating point numbers
;   * python types (fully defined with module name)
;   * references to other objects in the configuration, closed in <>
;   * strings (if it does not match any other pattern)
;   * tuples of the previous enclosed in brackets
;   * list of the previous, enclosed in square brackets, comma-separated
;
; The vocabularies are handled in a special way. If the vocabularies source is
; defined in the [main] (a dataset object) a dictionary that maps the language
; code to the vocabularies is created. Later, if any other block has a field
; called 'vocabulary', and its value is a known language code, the vocabulary
; from the dictionary is used. Vocabularies can be also defined as objects
; in the INI file and can be referenced using the <> notation.
;

[main]
; The main block contains the mandatory fields for running and experiment.
output=experiments/example-lm-$TIME
encoders=[]
decoder=<decoder>
runner=<runner>
evaluation=[<perplexity>]
threads=4
; The following options are used exclusively for training
name=language model
batch_size=5
epochs=10
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
minimize=True
validation_period=100
logging_period=20
save_n_best=1

[perplexity]
class=evaluators.perplexity.Perplexity

[train_data]
; This is definition of the training data object. Notice that language are
; defined here, because they are used identifiers while preparing vocabularies.
; Dataset is not a standard class, it treats the __init__ methods arguements as
; a dictionary, therefore the data series names can be any strings.
class=config.utils.dataset_from_files
s_target=examples/data/train.de

[val_data]
; Validation data, the languages are not necessary here, encoders and decoder
; acces the data series via the string identifiers defined here.
class=config.utils.dataset_from_files
s_target=examples/data/val.de

[decoder_vocabulary]
class=config.utils.initialize_vocabulary
directory=tmp-lm-decoder-vocabulary
name=decoder-vocabulary
datasets=[<train_data>]
series_ids=[target]
max_size=25000

[decoder]
class=decoders.decoder.Decoder
encoders=[]
rnn_size=256
embedding_size=256
use_attention=True
dropout_keep_p=0.5
data_id=target
vocabulary=<decoder_vocabulary>
max_output_len=50

[trainer]
; This block just fills the arguments of the trainer __init__ method.
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoder=<decoder>
l2_regularization=1.0e-8

[runner]
class=runners.perplexity_runner.PerplexityRunner
decoder=<decoder>
batch_size=256
